{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bec3d2ce-1e58-4971-b9b4-f56fe0f5bc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据形状: (11, 3468, 3928)\n",
      "展平后数据形状: torch.Size([11, 13622304])\n",
      "input_dim: 13622304\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2226805995220992 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 158\u001b[0m\n\u001b[0;32m    155\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(train_X, train_y)\n\u001b[0;32m    156\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 158\u001b[0m model \u001b[38;5;241m=\u001b[39m TransformerModel(\n\u001b[0;32m    159\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39minput_dim,\n\u001b[0;32m    160\u001b[0m     output_dim\u001b[38;5;241m=\u001b[39minput_dim,\n\u001b[0;32m    161\u001b[0m     nhead\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13622304\u001b[39m,\n\u001b[0;32m    162\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    163\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m    164\u001b[0m )\n\u001b[0;32m    166\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m  \u001b[38;5;66;03m# 学习率\u001b[39;00m\n\u001b[0;32m    167\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n",
      "Cell \u001b[1;32mIn[3], line 58\u001b[0m, in \u001b[0;36mTransformerModel.__init__\u001b[1;34m(self, input_dim, output_dim, nhead, num_layers, dropout)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28msuper\u001b[39m(TransformerModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformerEncoderLayer(\n\u001b[0;32m     59\u001b[0m     d_model\u001b[38;5;241m=\u001b[39minput_dim, nhead\u001b[38;5;241m=\u001b[39mnhead, dropout\u001b[38;5;241m=\u001b[39mdropout, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformerEncoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layer, num_layers\u001b[38;5;241m=\u001b[39mnum_layers)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(input_dim, output_dim)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:728\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.__init__\u001b[1;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, bias, device, dtype)\u001b[0m\n\u001b[0;32m    726\u001b[0m factory_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: device, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype}\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 728\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m MultiheadAttention(\n\u001b[0;32m    729\u001b[0m     d_model,\n\u001b[0;32m    730\u001b[0m     nhead,\n\u001b[0;32m    731\u001b[0m     dropout\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[0;32m    732\u001b[0m     bias\u001b[38;5;241m=\u001b[39mbias,\n\u001b[0;32m    733\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39mbatch_first,\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs,\n\u001b[0;32m    735\u001b[0m )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;66;03m# Implementation of Feedforward model\u001b[39;00m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1 \u001b[38;5;241m=\u001b[39m Linear(d_model, dim_feedforward, bias\u001b[38;5;241m=\u001b[39mbias, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1092\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[1;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_proj_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[1;32m-> 1092\u001b[0m         torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m embed_dim, embed_dim), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n\u001b[0;32m   1093\u001b[0m     )\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_proj_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk_proj_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2226805995220992 bytes."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "# 设置随机种子以确保结果可重复\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# -------------------\n",
    "# 1. 数据加载和预处理\n",
    "# -------------------\n",
    "def load_data(years, data_dir):\n",
    "    \"\"\"加载 ICIO 表格数据\"\"\"\n",
    "    data = []\n",
    "    for year in years:\n",
    "        file_path = os.path.join(data_dir, f\"{year}_SML.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"文件 {file_path} 不存在，请检查路径或文件名\")\n",
    "        df = pd.read_csv(file_path, header=0, index_col=0)  # 第一行和第一列为表头              \n",
    "        data.append(df.values)\n",
    "    return np.stack(data, axis=0)  # 返回形状为 (num_years, num_rows, num_cols)\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"数据预处理：展平并标准化\"\"\"\n",
    "    num_years, num_rows, num_cols = data.shape\n",
    "    input_dim = num_rows * num_cols\n",
    "    data_flat = data.reshape(num_years, input_dim)  # (num_years, input_dim)\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data_flat)\n",
    "    return torch.FloatTensor(data_scaled), scaler, input_dim\n",
    "\n",
    "# 数据路径和年份\n",
    "data_dir = \"./ICIO DATA\"  # 请替换为实际数据路径\n",
    "years = list(range(2010, 2021))  # 1995-2020，共 26 年\n",
    "data = load_data(years, data_dir)\n",
    "data_tensor, scaler, input_dim = preprocess_data(data)\n",
    "\n",
    "print(f\"数据形状: {data.shape}\")  # 应为 (26, num_rows, num_cols)\n",
    "print(f\"展平后数据形状: {data_tensor.shape}\")  # 应为 (26, input_dim)\n",
    "print(f\"input_dim: {input_dim}\")\n",
    "\n",
    "# -------------------\n",
    "# 2. Transformer 模型定义\n",
    "# -------------------\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, nhead, num_layers, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim, nhead=nhead, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        memory = self.encoder(src)  # (batch_size, seq_len, input_dim)\n",
    "        output = self.fc(memory[:, -1, :])  # 取最后一步输出，(batch_size, output_dim)\n",
    "        return output\n",
    "\n",
    "# -------------------\n",
    "# 3. 数据准备函数\n",
    "# -------------------\n",
    "def prepare_sequences(data_tensor, seq_len):\n",
    "    \"\"\"生成时间序列数据\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data_tensor) - seq_len):\n",
    "        X.append(data_tensor[i:i+seq_len])  # (seq_len, input_dim)\n",
    "        y.append(data_tensor[i+seq_len])    # (input_dim)\n",
    "    if len(X) == 0:\n",
    "        raise ValueError(f\"样本数为 0，请减少 seq_len（当前为 {seq_len}）或增加数据年份\")\n",
    "    return torch.stack(X), torch.stack(y)  # X: (num_samples, seq_len, input_dim), y: (num_samples, input_dim)\n",
    "\n",
    "# -------------------\n",
    "# 4. 超参数优化\n",
    "# -------------------\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna 优化目标函数\"\"\"\n",
    "    # 计算 input_dim 的所有因子\n",
    "    factors = [i for i in range(1, input_dim + 1) if input_dim % i == 0]\n",
    "    possible_nheads = [f for f in factors if f <= 8]  # 限制 nhead <= 8\n",
    "    if not possible_nheads:\n",
    "        possible_nheads = [1]  # 如果没有合适的因子，默认使用 1\n",
    "    nhead = trial.suggest_categorical(\"nhead\", possible_nheads)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "\n",
    "    # 初始化模型\n",
    "    model = TransformerModel(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=input_dim,\n",
    "        nhead=nhead,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 准备序列数据\n",
    "    seq_len = 5\n",
    "    X, y = prepare_sequences(data_tensor, seq_len)\n",
    "    num_samples = len(X)\n",
    "    train_size = int(0.8 * num_samples)\n",
    "    if train_size == 0:\n",
    "        raise ValueError(f\"训练集为空，num_samples={num_samples}，请减少 seq_len 或增加数据\")\n",
    "    print(f\"num_samples: {num_samples}, train_size: {train_size}\")\n",
    "\n",
    "    train_X, val_X = X[:train_size], X[train_size:]\n",
    "    train_y, val_y = y[:train_size], y[train_size:]\n",
    "    train_dataset = TensorDataset(train_X, train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # 训练模型\n",
    "    model.train()\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 验证\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(val_X)\n",
    "        val_loss = criterion(val_output, val_y).item()\n",
    "    return val_loss\n",
    "\n",
    "# 运行超参数优化\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "best_params = study.best_params\n",
    "print(\"最佳超参数:\", best_params)\n",
    "\n",
    "# -------------------\n",
    "# 5. 使用最佳参数训练完整模型\n",
    "# -------------------\n",
    "seq_len = 5\n",
    "X, y = prepare_sequences(data_tensor, seq_len)\n",
    "train_size = int(0.8 * len(X))\n",
    "train_X, val_X = X[:train_size], X[train_size:]\n",
    "train_y, val_y = y[:train_size], y[train_size:]\n",
    "train_dataset = TensorDataset(train_X, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "model = TransformerModel(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=input_dim,\n",
    "    nhead=best_params[\"nhead\"],\n",
    "    num_layers=best_params[\"num_layers\"],\n",
    "    dropout=best_params[\"dropout\"]\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# TensorBoard 可视化\n",
    "writer = SummaryWriter(\"runs/icio_transformer\")\n",
    "\n",
    "# 训练\n",
    "num_epochs = 50\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "    writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# -------------------\n",
    "# 6. 预测未来 ICIO 表\n",
    "# -------------------\n",
    "model.eval()\n",
    "future_years = 5  # 预测未来 5 年\n",
    "predictions = []\n",
    "last_sequence = data_tensor[-seq_len:].unsqueeze(0)  # (1, seq_len, input_dim)\n",
    "with torch.no_grad():\n",
    "    for _ in range(future_years):\n",
    "        pred = model(last_sequence)  # (1, input_dim)\n",
    "        predictions.append(pred.squeeze(0))\n",
    "        last_sequence = torch.cat((last_sequence[:, 1:, :], pred.unsqueeze(1)), dim=1)\n",
    "\n",
    "predictions = torch.stack(predictions)  # (future_years, input_dim)\n",
    "predictions_np = scaler.inverse_transform(predictions.numpy())  # 反标准化\n",
    "predictions_reshaped = predictions_np.reshape(future_years, data.shape[1], data.shape[2])\n",
    "\n",
    "# 导出预测结果\n",
    "for i, year in enumerate(range(2021, 2021 + future_years)):\n",
    "    pd.DataFrame(predictions_reshaped[i], index=data[0].index, columns=data[0].columns).to_csv(f\"pred_{year}.csv\")\n",
    "\n",
    "# -------------------\n",
    "# 7. 可视化\n",
    "# -------------------\n",
    "plt.plot(range(1995, 2021), data_tensor[:, 0], label=\"Historical Data (first feature)\")\n",
    "plt.plot(range(2021, 2021 + future_years), predictions[:, 0], label=\"Predicted (first feature)\")\n",
    "plt.legend()\n",
    "plt.title(\"Historical and Predicted ICIO Values\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.savefig(\"prediction_plot.png\")\n",
    "plt.show()\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
